---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
author:
  - name: Ling Lu
    affiliations:
      - id: U15738240
        name: Boston University
        city: Boston
        state: MA
  - name: Luoyan Zhang
    affiliations:
      - id: U00607539
        name: Boston University
        city: Boston
        state: MA
  - name: Yinuo Wang
    affiliations:
      - id: U78839500
        name: Boston University
        city: Boston
        state: MA
bibliography: references.bib
csl: csl/econometrica.csl
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
---


# Introduction

This section presents a detailed data analysis of job market trends in 2024, focusing on AI-driven changes, salary disparities, and employment trends across different regions and industries.


## Data Import and Cleaning
```python
# Load necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = pd.read_csv("lightcast_job_postings.csv")

# Display dataset summary
print(data.info())
print(data.describe())
```

# Data Cleaning & Preprocessing

## Drop Unnecessary Columns

 **Which columns should be dropped, and why?**

The columns selected for removal are considered redundant because they either provide duplicate information, are unnecessary for analysis, or have more detailed equivalents in the dataset. For example, `"ID"` serves as a unique identifier but is often not needed for analysis, while `"URL"` and `"ACTIVE_URLS"` contain job posting links that are useful externally but not critical for data processing. Similarly, `"LAST_UPDATED_TIMESTAMP"` is dropped because `"LAST_UPDATED_DATE"` already provides update information in a more readable format. The `"DUPLICATES"` column, which likely flags repeated entries, is also removed since duplicates can be handled separately.

Additionally, industry and occupational classification columns like `"NAICS2"` to `"NAICS6"` and `"SOC_2"`, `"SOC_3"`, `"SOC_5"` are removed because these represent different levels of classification, and more relevant or updated versions (e.g., `"NAICS_2022_2"` to `"NAICS_2022_6"`) are already present in the dataset. Removing these redundant columns helps streamline the dataset, making it more efficient to analyze without losing valuable information.

### Dropping Unnecessary Columns
```python
columns_to_drop = [
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    "NAICS2", "NAICS3", "NAICS4", "NAICS5", "NAICS6",
    "SOC_2", "SOC_3", "SOC_5"
]
df.drop(columns=columns_to_drop, inplace=True)
```

##  Handle Missing Values
 **How should missing values be handled?**

Missing values should be handled strategically based on their impact on analysis. First, visualizing missing data with a heatmap helps identify patterns and assess severity. Columns with more than 50% missing values are dropped to avoid unreliable or incomplete data. For numerical fields like `"Salary"`, filling missing values with the median ensures the data remains representative without being skewed by outliers. Categorical fields like `"Industry"` are filled with `"Unknown"` to maintain completeness while preserving interpretability. This approach balances data retention and accuracy, ensuring meaningful analysis without introducing bias.

```python
import missingno as msno
import matplotlib.pyplot as plt

# Visualize missing data
msno.heatmap(df)
plt.title("Missing Values Heatmap")
plt.show()

# Drop columns with >50% missing values
df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)

# Fill missing values
df["Salary"].fillna(df["Salary"].median(), inplace=True)
df["Industry"].fillna("Unknown", inplace=True)
```

## Remove Duplicates

To ensure each job is counted only once, we remove duplicates based on job title, company, location, and posting date.

```python
df = df.drop_duplicates(subset=["TITLE", "COMPANY", "LOCATION", "POSTED"], keep="first")
```

# Exploratory Data Analysis (EDA)
ðŸ“Œ **Why these visualizations were chosen?**

EDA helps uncover patterns in job postings and salaries across industries. These insights assist job seekers in making informed career decisions.

### Job Postings by Industry
**Why this visualization?**

The bar chart displays the distribution of job postings across different industries, helping identify which sectors have the most opportunities. This allows for a quick comparison of industry demand and can highlight trends in job availability.

```python
import plotly.express as px

# Job postings by industry
fig = px.bar(df["Industry"].value_counts(), title="Job Postings by Industry")
fig.show()
```

### Salary Distribution by Industry
**Why this visualization?**

The box plot visualizes salary distribution by industry, showcasing variations in compensation and identifying potential outliers. This helps compare earnings across different sectors and assess salary disparities within the job market.

```python
fig = px.box(df, x="Industry", y="Salary", title="Salary Distribution by Industry")
fig.show()
```

### Remote vs. On-Site Jobs
**Why this visualization?**

The pie chart illustrates the proportion of remote versus on-site jobs, providing insights into workplace flexibility. It helps understand the prevalence of remote work and how job location preferences are shifting in the job market.

```python
fig = px.pie(df, names="REMOTE_TYPE_NAME", title="Remote vs. On-Site Jobs")
fig.show()
```



## Conclusion
This analysis provides insights into the evolving job market in 2024, highlighting AI's impact, salary trends, and employment disparities. The findings indicate how different disciplines benefit from AI's rise, with certain fields seeing more substantial salary growth. Further analysis will explore regional job differences and future career recommendations.